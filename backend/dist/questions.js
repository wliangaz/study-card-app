"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.questions = void 0;
exports.questions = [
    // Basic Questions
    {
        id: 1,
        topic: 'pyspark',
        question: 'What is PySpark?',
        difficulty: 'basic',
        answer: 'PySpark is the Python API for Apache Spark, an open-source, distributed computing system used for big data processing.',
    },
    {
        id: 2,
        topic: 'pyspark',
        question: 'What is a SparkContext?',
        difficulty: 'basic',
        answer: 'The SparkContext is the entry point to any Spark functionality. It represents the connection to a Spark cluster and is used to create RDDs, accumulators, and broadcast variables.',
    },
    {
        id: 3,
        topic: 'pyspark',
        question: 'What is an RDD?',
        difficulty: 'basic',
        answer: 'An RDD (Resilient Distributed Dataset) is the fundamental data structure in Spark. It is an immutable, distributed collection of objects that can be processed in parallel.',
    },
    { id: 4, topic: 'pyspark', question: 'What are transformations in PySpark?', difficulty: 'basic', answer: 'A transformation is a function that produces a new RDD from the existing RDDs.' },
    { id: 5, topic: 'pyspark', question: 'What are actions in PySpark?', difficulty: 'basic', answer: 'An action is a function that returns a value to the driver program after running a computation on the RDD.' },
    { id: 6, topic: 'pyspark', question: 'What is lazy evaluation?', difficulty: 'basic', answer: 'Lazy evaluation means that Spark does not evaluate transformations as they are called. Instead, it builds up a plan of transformations, and then executes them all at once when an action is called.' },
    { id: 7, topic: 'pyspark', question: 'How do you create an RDD?', difficulty: 'basic', answer: 'You can create an RDD by parallelizing an existing collection in your driver program, or by referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.' },
    { id: 8, topic: 'pyspark', question: 'What is the purpose of the `collect()` action?', difficulty: 'basic', answer: 'The `collect()` action returns all the elements of the RDD as an array at the driver program. This is useful when you want to view the entire dataset, but can be dangerous if the dataset is very large.' },
    { id: 9, topic: 'pyspark', question: 'What is the purpose of the `count()` action?', difficulty: 'basic', answer: 'The `count()` action returns the number of elements in the RDD.' },
    { id: 10, topic: 'pyspark', question: 'What is a DataFrame in PySpark?', difficulty: 'basic', answer: 'A DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood.' },
    { id: 11, topic: 'pyspark', question: 'How do you create a DataFrame?', difficulty: 'basic', answer: 'You can create a DataFrame from a wide range of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.' },
    { id: 12, topic: 'pyspark', question: 'What is a Spark Session?', difficulty: 'basic', answer: 'A Spark Session is the entry point to programming Spark with the Dataset and DataFrame API. It provides a single point of entry to interact with underlying Spark functionality and allows programming Spark with DataFrame and Dataset APIs. ' },
    { id: 13, topic: 'pyspark', question: 'How do you show the contents of a DataFrame?', difficulty: 'basic', answer: 'You can use the `show()` method to display the contents of a DataFrame in a tabular format.' },
    { id: 14, topic: 'pyspark', question: 'What is the `printSchema()` method used for?', difficulty: 'basic', answer: 'The `printSchema()` method prints the schema of the DataFrame to the console in a tree format.' },
    { id: 15, topic: 'pyspark', question: 'How do you select columns from a DataFrame?', difficulty: 'basic', answer: 'You can use the `select()` method to select a set of columns from a DataFrame.' },
    { id: 16, topic: 'pyspark', question: 'How do you filter data in a DataFrame?', difficulty: 'basic', answer: 'You can use the `filter()` or `where()` method to filter rows from a DataFrame based on a given condition.' },
    { id: 17, topic: 'pyspark', question: 'What is the `groupBy()` operation?', difficulty: 'basic', answer: 'The `groupBy()` operation is used to group the DataFrame based on some columns, so we can perform aggregation on them.' },
    { id: 18, topic: 'pyspark', question: 'How do you add a new column to a DataFrame?', difficulty: 'basic', answer: 'You can use the `withColumn()` method to add a new column to a DataFrame.' },
    { id: 19, topic: 'pyspark', question: 'How do you read a CSV file into a DataFrame?', difficulty: 'basic', answer: 'You can use the `spark.read.csv()` method to read a CSV file into a PySpark DataFrame.' },
    { id: 20, topic: 'pyspark', question: 'How do you write a DataFrame to a Parquet file?', difficulty: 'basic', answer: 'You can use the `df.write.parquet()` method to write a DataFrame to a Parquet file.' },
    { id: 21, topic: 'pyspark', question: 'What is Spark SQL?', difficulty: 'basic', answer: 'Spark SQL is a Spark module for structured data processing. It provides a programming abstraction called DataFrames and can also act as a distributed SQL query engine.' },
    { id: 22, topic: 'pyspark', question: 'How do you register a DataFrame as a temporary table?', difficulty: 'basic', answer: 'You can use the `createOrReplaceTempView()` method to register a DataFrame as a temporary table.' },
    { id: 23, topic: 'pyspark', question: 'What is the difference between `sort()` and `orderBy()`?', difficulty: 'basic', answer: 'They are aliases for each other and do the same thing.' },
    { id: 24, topic: 'pyspark', question: 'What is the `limit()` function used for?', difficulty: 'basic', answer: 'The `limit()` function is used to restrict the number of rows returned from a DataFrame.' },
    { id: 25, topic: 'pyspark', question: 'What is the purpose of caching a DataFrame?', difficulty: 'basic', answer: 'The purpose of caching a DataFrame is to store it in memory so that it can be accessed faster in subsequent operations.' },
    // Mid-level Questions
    { id: 26, topic: 'pyspark', question: 'Explain the difference between `map()` and `flatMap()`.', difficulty: 'mid', answer: '`map()` transforms each element of an RDD into one new element, creating a new RDD of the same size. `flatMap()` transforms each element into zero or more new elements, so the new RDD can be of a different size.' },
    { id: 27, topic: 'pyspark', question: 'Explain the difference between `reduce()` and `reduceByKey()`.', difficulty: 'mid', answer: '`reduce()` is an action that aggregates all the elements of an RDD into a single value. `reduceByKey()` is a transformation that aggregates the values for each key in a Pair RDD.' },
    { id: 28, topic: 'pyspark', question: 'What is a Pair RDD?', difficulty: 'mid', answer: 'A Pair RDD is a special type of RDD where each element is a key-value pair. They are useful for performing operations that involve grouping or aggregating data by key.' },
    { id: 29, topic: 'pyspark', question: 'What are accumulators in PySpark?', difficulty: 'mid', answer: 'Accumulators are variables that are only "added" to through an associative and commutative operation and can therefore be efficiently supported in parallel. They can be used to implement counters (as in MapReduce) or sums.' },
    { id: 30, topic: 'pyspark', question: 'What are broadcast variables in PySpark?', difficulty: 'mid', answer: 'Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. They can be used, for example, to give every node a copy of a large input dataset in an efficient manner.' },
    { id: 31, topic: 'pyspark', question: 'Explain the concept of partitioning in PySpark.', difficulty: 'mid', answer: 'Partitioning is the process of dividing a large dataset into smaller, more manageable parts called partitions. This allows Spark to process the data in parallel across multiple nodes in a cluster.' },
    { id: 32, topic: 'pyspark', question: 'How can you repartition an RDD?', difficulty: 'mid', answer: 'You can use the `repartition()` or `coalesce()` transformation to change the number of partitions of an RDD.' },
    { id: 33, topic: 'pyspark', question: 'What is the difference between `repartition()` and `coalesce()`?', difficulty: 'mid', answer: '`repartition()` can be used to either increase or decrease the number of partitions, and it performs a full shuffle of the data. `coalesce()` can only be used to decrease the number of partitions, and it avoids a full shuffle by combining existing partitions.' },
    { id: 34, topic: 'pyspark', question: 'What is a cogroup transformation?', difficulty: 'mid', answer: 'The `cogroup` transformation groups data from two RDDs that share the same key.' },
    { id: 35, topic: 'pyspark', question: 'What is the difference between `join()`, `leftOuterJoin()`, and `rightOuterJoin()`?', difficulty: 'mid', answer: '`join()` returns a new RDD containing all pairs of elements with matching keys in both RDDs. `leftOuterJoin()` returns a new RDD containing all pairs of elements with matching keys in both RDDs, and all elements from the left RDD that do not have a matching key in the right RDD. `rightOuterJoin()` is similar to `leftOuterJoin()`, but for the right RDD.' },
    { id: 36, topic: 'pyspark', question: 'What are User Defined Functions (UDFs) in PySpark?', difficulty: 'mid', answer: 'User Defined Functions (UDFs) are functions created by the user to perform a specific task that is not available in the built-in functions of PySpark.' },
    { id: 37, topic: 'pyspark', question: 'How do you create and use a UDF?', difficulty: 'mid', answer: 'You can create a UDF by defining a Python function and then wrapping it with the `udf()` function from the `pyspark.sql.functions` module. You can then use the UDF in a DataFrame transformation.' },
    { id: 38, topic: 'pyspark', question: 'What are the performance implications of using UDFs?', difficulty: 'mid', answer: 'UDFs can be slower than built-in functions because Spark has to serialize and deserialize the data to and from the Python process. It is recommended to use built-in functions whenever possible.' },
    { id: 39, topic: 'pyspark', question: 'What is the Catalyst Optimizer?', difficulty: 'mid', answer: 'The Catalyst Optimizer is a query optimizer in Spark SQL that performs rule-based and cost-based optimization to improve the performance of Spark jobs.' },
    { id: 40, topic: 'pyspark', question: 'Explain the architecture of a Spark application.', difficulty: 'mid', answer: 'A Spark application consists of a driver program and a set of executors. The driver program is responsible for creating the SparkContext, which in turn connects to a cluster manager to request resources. The executors are responsible for running the tasks that make up the Spark job.' },
    { id: 41, topic: 'pyspark', question: 'What is the role of the Cluster Manager?', difficulty: 'mid', answer: 'The Cluster Manager is responsible for allocating resources to Spark applications. Spark supports several cluster managers, including Standalone, YARN, and Mesos.' },
    { id: 42, topic: 'pyspark', question: 'What are some common file formats used with PySpark?', difficulty: 'mid', answer: 'Some common file formats used with PySpark include CSV, JSON, Parquet, and ORC.' },
    { id: 43, topic: 'pyspark', question: 'Explain how to handle missing or null data in a DataFrame.', difficulty: 'mid', answer: 'You can use the `na` functions in PySpark to handle missing or null data. For example, you can use `na.fill()` to replace null values with a specific value, or `na.drop()` to remove rows with null values.' },
    { id: 44, topic: 'pyspark', question: 'What is the difference between `na.fill()` and `na.drop()`?', difficulty: 'mid', answer: '`na.fill()` replaces null values with a specified value, while `na.drop()` removes rows that contain null values.' },
    { id: 45, topic: 'pyspark', question: 'What is the `withColumnRenamed()` method used for?', difficulty: 'mid', answer: 'The `withColumnRenamed()` method is used to rename a column in a DataFrame.' },
    { id: 46, topic: 'pyspark', question: 'How do you perform aggregations on a DataFrame?', difficulty: 'mid', answer: 'You can use the `groupBy()` method followed by an aggregation function (e.g., `sum()`, `avg()`, `count()`) to perform aggregations on a DataFrame.' },
    { id: 47, topic: 'pyspark', question: 'What is the `pivot()` function?', difficulty: 'mid', answer: 'The `pivot()` function is used to rotate a DataFrame from a long format to a wide format.' },
    { id: 48, topic: 'pyspark', question: 'What are window functions in PySpark?', difficulty: 'mid', answer: 'Window functions are used to perform calculations across a set of rows that are related to the current row. They are similar to aggregation functions, but they do not collapse the rows.' },
    { id: 49, topic: 'pyspark', question: 'Explain the use of `rank()` and `dense_rank()`.', difficulty: 'mid', answer: '`rank()` and `dense_rank()` are window functions that assign a rank to each row in a partition. `rank()` assigns a unique rank to each row, while `dense_rank()` assigns the same rank to rows that have the same value.' },
    { id: 50, topic: 'pyspark', question: 'What is the difference between a DataFrame and a Dataset?', difficulty: 'mid', answer: 'A DataFrame is a distributed collection of data organized into named columns. A Dataset is a distributed collection of data that is strongly typed. Datasets provide compile-time type safety, which means that you can catch errors at compile time rather than at runtime.' },
    // Advanced Questions
    { id: 51, topic: 'pyspark', question: 'Explain the concept of data locality.', difficulty: 'advanced', answer: 'Data locality is the concept of moving the computation close to the data, rather than moving the data to the computation. This is important in distributed computing because it can significantly reduce network traffic and improve performance.' },
    { id: 52, topic: 'pyspark', question: 'What is a shuffle in PySpark and why is it expensive?', difficulty: 'advanced', answer: 'A shuffle is the process of redistributing data across the partitions of an RDD. Shuffles are expensive because they involve a lot of network I/O and disk I/O.' },
    { id: 53, topic: 'pyspark', question: 'What are some ways to minimize shuffling?', difficulty: 'advanced', answer: 'Some ways to minimize shuffling include using broadcast variables, using accumulators, and using the `reduceByKey()` transformation instead of the `groupByKey()` transformation.' },
    { id: 54, topic: 'pyspark', question: 'Explain the difference between wide and narrow transformations.', difficulty: 'advanced', answer: 'Narrow transformations are transformations where each partition of the parent RDD is used by at most one partition of the child RDD. Wide transformations are transformations where multiple child RDD partitions may depend on a single parent RDD partition.' },
    { id: 55, topic: 'pyspark', question: 'What is the Tungsten execution engine?', difficulty: 'advanced', answer: 'Tungsten is a Spark execution engine that uses code generation to optimize Spark jobs. It can significantly improve the performance of Spark jobs by reducing the amount of memory that is used and by reducing the number of CPU cycles that are required.' },
    { id: 56, topic: 'pyspark', question: 'What is speculative execution?', difficulty: 'advanced', answer: 'Speculative execution is a feature in Spark that allows it to run multiple copies of the same task on different nodes in a cluster. If one of the copies fails, Spark can use the results from one of the other copies to complete the task.' },
    { id: 57, topic: 'pyspark', question: 'How does Spark handle fault tolerance?', difficulty: 'advanced', answer: 'Spark handles fault tolerance by using RDDs, which are resilient distributed datasets. RDDs are immutable, so if a partition of an RDD is lost, Spark can recompute it from the original data.' },
    { id: 58, topic: 'pyspark', question: 'What is lineage in Spark?', difficulty: 'advanced', answer: 'Lineage is the sequence of transformations that were used to create an RDD. Spark uses lineage to recompute lost partitions of an RDD.' },
    { id: 59, topic: 'pyspark', question: 'What is checkpointing and how is it different from caching?', difficulty: 'advanced', answer: 'Checkpointing is the process of saving an RDD to a reliable distributed file system, such as HDFS. Caching is the process of storing an RDD in memory. Checkpointing is more reliable than caching, but it is also slower.' },
    { id: 60, topic: 'pyspark', question: 'Explain the different storage levels for caching.', difficulty: 'advanced', answer: 'The different storage levels for caching are `MEMORY_ONLY`, `MEMORY_ONLY_SER`, `MEMORY_AND_DISK`, `MEMORY_AND_DISK_SER`, `DISK_ONLY`, and `OFF_HEAP`.' },
    { id: 61, topic: 'pyspark', question: 'What is dynamic allocation in Spark?', difficulty: 'advanced', answer: 'Dynamic allocation is a feature in Spark that allows it to dynamically adjust the number of executors that are used by a Spark application. This can be useful for applications that have a variable workload.' },
    { id: 62, topic: 'pyspark', question: 'How do you monitor and debug a PySpark application?', difficulty: 'advanced', answer: 'You can use the Spark UI to monitor and debug a PySpark application. The Spark UI provides a wealth of information about the application, including the status of the jobs, the stages, and the tasks.' },
    { id: 63, topic: 'pyspark', question: 'What is the Spark UI and what can you learn from it?', difficulty: 'advanced', answer: 'The Spark UI is a web-based user interface that you can use to monitor and debug a Spark application. You can learn a lot from the Spark UI, including the status of the jobs, the stages, and the tasks.' },
    { id: 64, topic: 'pyspark', question: 'What are some common performance tuning techniques in PySpark?', difficulty: 'advanced', answer: 'Some common performance tuning techniques in PySpark include using broadcast variables, using accumulators, using the `reduceByKey()` transformation instead of the `groupByKey()` transformation, and caching RDDs.' },
    { id: 65, topic: 'pyspark', question: 'Explain how to handle data skew.', difficulty: 'advanced', answer: 'Data skew is a problem that can occur in distributed computing when the data is not evenly distributed across the partitions of an RDD. This can lead to some tasks taking much longer to complete than other tasks. You can handle data skew by using a salting technique.' },
    { id: 66, topic: 'pyspark', question: 'What is salting in the context of data skew?', difficulty: 'advanced', answer: 'Salting is a technique that can be used to handle data skew. It involves adding a random key to each element of the RDD. This will cause the data to be more evenly distributed across the partitions of the RDD.' },
    { id: 67, topic: 'pyspark', question: 'What is the difference between `broadcast join` and `shuffle hash join`?', difficulty: 'advanced', answer: 'A broadcast join is a type of join that can be used when one of the DataFrames is small enough to fit in memory. A shuffle hash join is a type of join that can be used when both of the DataFrames are large.' },
    { id: 68, topic: 'pyspark', question: 'When would you use a broadcast join?', difficulty: 'advanced', answer: 'You would use a broadcast join when one of the DataFrames is small enough to fit in memory. This is because a broadcast join is much more efficient than a shuffle hash join.' },
    { id: 69, topic: 'pyspark', question: 'What is Structured Streaming?', difficulty: 'advanced', answer: 'Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data.' },
    { id: 70, topic: 'pyspark', question: 'Explain the key concepts of Structured Streaming.', difficulty: 'advanced', answer: 'The key concepts of Structured Streaming are the DataFrame and Dataset APIs, the event-time processing, and the exactly-once fault-tolerance guarantees.' },
    { id: 71, topic: 'pyspark', question: 'What are the different output modes in Structured Streaming?', difficulty: 'advanced', answer: 'The different output modes in Structured Streaming are `append`, `complete`, and `update`.' },
    { id: 72, topic: 'pyspark', question: 'What is a trigger in Structured Streaming?', difficulty: 'advanced', answer: 'A trigger is a mechanism that controls when a streaming query is executed. You can use a trigger to specify that a query should be executed every time a new batch of data arrives, or you can use a trigger to specify that a query should be executed at a fixed interval.' },
    { id: 73, topic: 'pyspark', question: 'What is MLlib?', difficulty: 'advanced', answer: 'MLlib is Spark\'s machine learning library. It provides a wide range of machine learning algorithms, including classification, regression, clustering, and collaborative filtering.' },
    { id: 74, topic: 'pyspark', question: 'Explain the components of a MLlib pipeline.', difficulty: 'advanced', answer: 'The components of an MLlib pipeline are a sequence of stages, where each stage is either a Transformer or an Estimator. A Transformer is an algorithm which can transform one DataFrame into another DataFrame. An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer.' },
    { id: 75, topic: 'pyspark', question: 'What is GraphFrames?', difficulty: 'advanced', answer: 'GraphFrames is a package for Apache Spark which provides DataFrame-based Graphs. It provides a set of APIs for performing graph analysis.' },
    // Expert Questions
    { id: 76, topic: 'pyspark', question: 'Explain the internals of the Catalyst Optimizer.', difficulty: 'expert', answer: 'The Catalyst Optimizer is a query optimizer in Spark SQL that performs rule-based and cost-based optimization to improve the performance of Spark jobs. It has four main phases: analysis, logical optimization, physical planning, and code generation.' },
    { id: 77, topic: 'pyspark', question: 'What is Project Tungsten and how does it improve performance?', difficulty: 'expert', answer: 'Project Tungsten is a Spark project that aims to improve the performance of Spark jobs by optimizing memory and CPU usage. It does this by using a variety of techniques, including code generation, cache-aware computation, and explicit memory management.' },
    { id: 78, topic: 'pyspark', question: 'Explain how memory is managed in Spark.', difficulty: 'expert', answer: 'Memory in Spark is managed by the Spark memory manager. The memory manager is responsible for allocating and deallocating memory to the various components of a Spark application.' },
    { id: 79, topic: 'pyspark', question: 'What is the difference between on-heap and off-heap memory?', difficulty: 'expert', answer: 'On-heap memory is memory that is managed by the Java Virtual Machine (JVM). Off-heap memory is memory that is managed by the operating system. Off-heap memory can be faster than on-heap memory, but it is also more difficult to manage.' },
    { id: 80, topic: 'pyspark', question: 'How do you configure memory settings in Spark?', difficulty: 'expert', answer: 'You can configure memory settings in Spark by setting the `spark.driver.memory` and `spark.executor.memory` properties.' },
    { id: 81, topic: 'pyspark', question: 'What is garbage collection in the context of Spark?', difficulty: 'expert', answer: 'Garbage collection is the process of reclaiming memory that is no longer being used by a program. In the context of Spark, garbage collection can be a major performance bottleneck. This is because Spark applications can create a lot of objects, and the garbage collector can have a difficult time keeping up.' },
    { id: 82, topic: 'pyspark', question: 'How can you optimize garbage collection in a Spark application?', difficulty: 'expert', answer: 'You can optimize garbage collection in a Spark application by using a variety of techniques, including using a generational garbage collector, tuning the garbage collector settings, and using off-heap memory.' },
    { id: 83, topic: 'pyspark', question: 'What is the role of the DAGScheduler and TaskScheduler?', difficulty: 'expert', answer: 'The DAGScheduler is responsible for breaking down a Spark job into a set of stages. The TaskScheduler is responsible for scheduling the tasks that make up a stage.' },
    { id: 84, topic: 'pyspark', question: 'Explain the process of a Spark job execution from start to finish.', difficulty: 'expert', answer: 'A Spark job execution starts with the driver program creating a SparkContext. The SparkContext then connects to a cluster manager to request resources. The cluster manager then allocates resources to the Spark application. The driver program then sends the Spark job to the executors. The executors then run the tasks that make up the Spark job. The results of the tasks are then sent back to the driver program.' },
    { id: 85, topic: 'pyspark', question: 'What are some advanced techniques for dealing with data skew?', difficulty: 'expert', answer: 'Some advanced techniques for dealing with data skew include using a salting technique, using a broadcast join, and using a custom partitioner.' },
    { id: 86, topic: 'pyspark', question: 'How can you implement a custom partitioner?', difficulty: 'expert', answer: 'You can implement a custom partitioner by extending the `org.apache.spark.Partitioner` class.' },
    { id: 87, topic: 'pyspark', question: 'What are the challenges of using PySpark in a production environment?', difficulty: 'expert', answer: 'Some of the challenges of using PySpark in a production environment include managing dependencies, monitoring and debugging applications, and ensuring the security of the application.' },
    { id: 88, topic: 'pyspark', question: 'Explain how to secure a Spark application.', difficulty: 'expert', answer: 'You can secure a Spark application by using a variety of techniques, including using authentication, authorization, and encryption.' },
    { id: 89, topic: 'pyspark', question: 'What is dynamic partition pruning?', difficulty: 'expert', answer: 'Dynamic partition pruning is a feature in Spark that allows it to prune partitions from a table that are not needed for a query. This can significantly improve the performance of queries that access a small subset of the partitions in a table.' },
    { id: 90, topic: 'pyspark', question: 'What is adaptive query execution (AQE)?', difficulty: 'expert', answer: 'Adaptive query execution (AQE) is a feature in Spark that allows it to dynamically adjust the execution plan of a query based on the runtime statistics of the query. This can significantly improve the performance of queries that have a complex execution plan.' },
    { id: 91, topic: 'pyspark', question: 'How does AQE optimize Spark jobs?', difficulty: 'expert', answer: 'AQE optimizes Spark jobs by using a variety of techniques, including dynamically coalescing shuffle partitions, dynamically switching join strategies, and dynamically optimizing skew joins.' },
    { id: 92, topic: 'pyspark', question: 'What are some of the limitations of PySpark compared to Scala Spark?', difficulty: 'expert', answer: 'Some of the limitations of PySpark compared to Scala Spark include the performance of UDFs, the lack of support for some of the more advanced features of Spark, and the smaller community.' },
    { id: 93, topic: 'pyspark', question: 'How can you overcome the performance limitations of Python UDFs?', difficulty: 'expert', answer: 'You can overcome the performance limitations of Python UDFs by using a variety of techniques, including using Pandas UDFs, using Cython, and using Numba.' },
    { id: 94, topic: 'pyspark', question: 'What are Pandas UDFs (Vectorized UDFs)?', difficulty: 'expert', answer: 'Pandas UDFs are a type of UDF that can be used to perform vectorized operations on DataFrames. They can significantly improve the performance of UDFs by reducing the amount of data that needs to be serialized and deserialized.' },
    { id: 95, topic: 'pyspark', question: 'How do Pandas UDFs improve performance?', difficulty: 'expert', answer: 'Pandas UDFs improve performance by reducing the amount of data that needs to be serialized and deserialized. They do this by using Apache Arrow to transfer data between the JVM and the Python process.' },
    { id: 96, topic: 'pyspark', question: 'Explain how to use PySpark with a distributed storage system like HDFS.', difficulty: 'expert', answer: 'You can use PySpark with a distributed storage system like HDFS by using the `pyspark.sql.SparkSession.read.format()` method.' },
    { id: 97, topic: 'pyspark', question: 'What are some best practices for writing efficient PySpark code?', difficulty: 'expert', answer: 'Some best practices for writing efficient PySpark code include using broadcast variables, using accumulators, using the `reduceByKey()` transformation instead of the `groupByKey()` transformation, and caching RDDs.' },
    { id: 98, topic: 'pyspark', question: 'How do you handle large datasets that do not fit in memory?', difficulty: 'expert', answer: 'You can handle large datasets that do not fit in memory by using a variety of techniques, including using a distributed storage system like HDFS, using a streaming algorithm, and using a sampling technique.' },
    { id: 99, topic: 'pyspark', question: 'What are some common pitfalls to avoid when using PySpark?', difficulty: 'expert', answer: 'Some common pitfalls to avoid when using PySpark include using the `collect()` action on a large RDD, using the `groupByKey()` transformation instead of the `reduceByKey()` transformation, and not caching RDDs.' },
    { id: 100, topic: 'pyspark', question: 'Describe a complex PySpark project you have worked on.', difficulty: 'expert', answer: 'This is an open-ended question. A good answer would describe a project that involved a variety of challenges, such as working with large datasets, dealing with data skew, and tuning the performance of the application.' }
];
